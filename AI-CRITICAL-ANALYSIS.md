# Critical Analysis Report: AI-Generated vs Human-Written Spring Boot Code

## Methodology

### Testing Approach

To fairly compare the three Spring Boot applications (Human, GPT, and Gemini), we created identical integration test suites focusing on **controller-level tests**. These tests verify the external REST and GraphQL APIs that users interact with, making them the most meaningful comparison point.

### Why Controller Tests?

Controller tests are the best basis for comparison because:
- They test the **public API** that end-users actually interact with
- They verify **functional completeness** against the specification
- They are **independent** of internal implementation details
- All three apps expose the same REST endpoints, making tests directly comparable

Internal tests (repository, service) would require different implementations for each AI app due to varying internal APIs, making fair comparison difficult.

### Test Locations and Structure

**Human App Tests (Baseline):**
- Location: `src/test/java/com/example/spaceapp/controller/`
- Test files used for comparison:
  - `PlanetControllerIntegrationTest.java` (5 tests)
  - `MoonControllerIntegrationTest.java` (4 tests)
  - `UserGraphqlControllerIntegrationTest.java` (1 test)
- **Total: 10 controller integration tests**

**GPT App Tests:**
- Location: `gpt5.1_space-app/src/test/java/com/example/spaceapp/controller/`
- Test files (mirror human app):
  - `PlanetControllerIntegrationTest.java` (5 tests)
  - `MoonControllerIntegrationTest.java` (3 tests)
  - `UserGraphqlControllerIntegrationTest.java` (1 test)
- Utility files:
  - `testutil/TestUsers.java` (credentials helper)
  - `testutil/JsonUtil.java` (JSON serialization)
- **Total: 9 controller integration tests** (1 test skipped due to missing endpoint)

**Gemini App Tests:**
- Location: `Gemini3.1/universe-api/src/test/java/com/example/universe/controller/`
- Test files (adapted for Gemini's package structure):
  - `PlanetControllerIntegrationTest.java` (3 tests)
  - `MoonControllerIntegrationTest.java` (3 tests)
  - `UserGraphqlControllerIntegrationTest.java` (1 test)
- Utility files:
  - `testutil/TestUsers.java` (credentials helper)
  - `testutil/JsonUtil.java` (JSON serialization)
- **Total: 7 controller integration tests** (2 tests skipped due to missing endpoints)

### Test Coverage (Compared Across All Apps)

Each controller test suite covers:
1. **Authentication** - Unauthenticated requests return 401
2. **Role-based access** - Students can read, staff/admin can write
3. **CRUD operations** - Create, read, update, delete for planets and moons
4. **Query endpoints** - Search by type, get names, get specific fields (where implemented)
5. **GraphQL security** - Admin-only access to user management
6. **Validation** - Invalid data returns 400 with structured errors
7. **Error handling** - Missing resources return 404 with structured errors

### Test Execution

All tests were run using Maven:
```bash
mvn test
```

Results were captured from test output showing pass/fail status, HTTP status codes, and error messages.

---

## Introduction

This report compares three Spring Boot applications: one written by human developers (our team) and two generated by AI tools (ChatGPT and Google Gemini). All three apps implement the same specification for a space exploration API with planets, moons, and users. The goal is to see how well AI-generated code works compared to human code.

---

## GenAI Code Generation Process

### AI Tools Used

1. **ChatGPT** (GPT-4 model) - for the first AI implementation
2. **Google Gemini AI Studio** (Gemini 1.5) - for the second AI implementation

### Initial Prompt

We used the same initial prompt for both AI tools to ensure a fair comparison:

```
Generate a complete Spring Boot 3.5 project (Java 17, Maven) with REST API and GraphQL:

Functionality:
Entities:
Planet: id, name, type, radiusKm, massKg, orbitalPeriodDays,
Moon: id, name, diameterKm, orbitalPeriodDays, planetId (FK to Planet),
User: id, username, password (hashed), role (ADMIN/STAFF/STUDENT),
,
REST:
CRUD for planets and moons,
Extra queries: find planets by type, retrieve specific fields, list moons by planet name, count moons for a planet,
,
GraphQL:
Query: find user by ID,
Mutation: create user,
,

Requirements:
Use DTOs for input/output, with Jakarta validation (@NotNull, @Size, @Positive),
Centralized exception handling with @ControllerAdvice,
Logging with AspectJ (3 pointcuts: controllers, services, exceptions),
Security: Spring Security Basic Authentication with role-based access control
ADMIN: full access including user management,
STAFF: create/update/delete planets and moons,
STUDENT: read-only access,
,
Passwords stored with BCrypt hashing,
Controllers must use @ResponseStatus instead of ResponseEntity,
API documentation via Swagger/OpenAPI,
H2 database with preloaded sample data,

Format of the result:
I expect the entire project, with all key files required for build, delivered as a ZIP archive.,
After unzipping and running mvn clean install or mvn spring-boot:run, the project must compile and run successfully.
```

### Testing Process

Both AI-generated projects were:
1. **Extracted** from ZIP archives
2. **Built** using `mvn clean install`
3. **Run** using `mvn spring-boot:run`
4. **Tested** with identical integration test suites created for comparison
5. **Analyzed** for completeness, code quality, and adherence to requirements

### Results Summary

- **GPT Project**: Compiled and ran successfully. Generated as a ZIP archive with all Maven files. Missing 1 endpoint but otherwise ~90% complete.
- **Gemini Project**: Compiled and ran successfully. Generated as a complete project structure. Missing 2 endpoints and has a GraphQL security gap, ~85% complete.

---

## 1. Functionality Coverage

### 1.1 REST CRUD Operations

**What We Tested:** Testing if you can create, read, update, and delete planets and moons with proper authentication.

**Test Used:** `PlanetControllerIntegrationTest.staff_canCreateUpdateDelete()` and `MoonControllerIntegrationTest.staff_canCreateUpdateDelete_andQueries()`

**Results:**

| App | Create (POST) | Read (GET) | Update (PUT) | Delete |
|-----|---------------|------------|--------------|---------|
| **Human** | ✅ Returns 201 | ✅ Works | ✅ Works | ✅ Returns 204 |
| **GPT** | ✅ Returns 201 | ✅ Works | ✅ Works | ✅ Returns 204 |
| **Gemini** | ✅ Returns 201 | ✅ Works | ❌ Returns 500 (moon update) | ✅ Returns 204 |

**Summary:** Human and GPT apps have fully working CRUD. Gemini app mostly works but has a bug in moon update logic.

**Why This Matters:** Basic CRUD is the foundation of any REST API. If these operations don't work, nothing else matters.

---

### 1.2 Extra Query Endpoints

**What We Tested:** Special search features like finding planets by type, getting just planet names, and retrieving specific fields (name and mass).

**Test Used:** `PlanetControllerIntegrationTest.searchAndNamesEndpoints_shouldWork()` and `nameAndMassEndpoint_shouldReturnExpectedFields()`

**Results:**

| App | Search by Type | Get Names | Get Name+Mass |
|-----|----------------|-----------|---------------|
| **Human** | ✅ `/api/planets/search/by-type` | ✅ `/api/planets/names` | ✅ `/api/planets/fields/name-mass` |
| **GPT** | ✅ Works | ✅ Works | ❌ Missing endpoint |
| **Gemini** | ✅ Works (via `?type=` param) | ❌ Missing endpoint | ❌ Missing endpoint |

**Summary:** Human app has all required endpoints. GPT is missing one endpoint. Gemini is missing two endpoints and uses a different URL style (query parameters instead of paths).

**Why This Matters:** The specification lists specific endpoints that must be present. Missing endpoints means the app is incomplete.

---

### 1.3 GraphQL Operations

**What We Tested:** Can users query and create other users via GraphQL? Is it properly restricted to admins only?

**Test Used:** `UserGraphqlControllerIntegrationTest.graphql_userById_and_createUser_security()`

**Results:**

| App | Query Access | Mutation Access | Student Access |
|-----|--------------|-----------------|----------------|
| **Human** | ✅ Admin only | ✅ Admin only | ❌ Blocked (403) |
| **GPT** | ✅ Admin only | ✅ Admin only | ❌ Blocked (403) |
| **Gemini** | ❌ Any authenticated user | ✅ Admin only | ✅ Can query users |

**Summary:** Human and GPT apps correctly restrict GraphQL to admins. Gemini allows students to query users, which violates the specification.

**Why This Matters:** User information is sensitive. The spec says only admins should access user management, but Gemini lets anyone with a login see user data.

---

## 2. Code Quality

### 2.1 DTO Usage and Validation

**What We Checked:** Do the apps use separate Data Transfer Objects (DTOs) with validation rules?

**Inspection:** Reviewed `PlanetCreateUpdateDto`, `MoonCreateUpdateDto` files

**Results:**

| App | DTOs Present | Validation Annotations | Code Style |
|-----|--------------|----------------------|------------|
| **Human** | ✅ Yes | ✅ `@NotNull`, `@Size`, `@Positive` | Lombok `@Builder` |
| **GPT** | ✅ Yes | ✅ `@NotNull`, `@Size`, `@Positive` | Plain getters/setters |
| **Gemini** | ✅ Yes | ✅ `@NotNull`, `@Size`, `@Positive` | Lombok `@Builder` |

**Summary:** All three apps properly use DTOs with validation. Human and Gemini use Lombok to reduce boilerplate code, GPT uses plain Java which is more verbose but still correct.

**Why This Matters:** DTOs separate internal models from external API, making code cleaner and more secure. Validation prevents bad data from entering the system.

---

### 2.2 Service Layer Logic

**What We Checked:** Is business logic properly separated into service classes instead of being in controllers?

**Inspection:** Reviewed `PlanetService`, `MoonService` structure

**Results:**

| App | Service Layer | Validation Logic | DTO Mapping | Issues |
|-----|---------------|------------------|-------------|---------|
| **Human** | ✅ Clear methods | ✅ Duplicate name check | ✅ toDto/fromDto | None |
| **GPT** | ✅ Similar structure | ✅ Duplicate name check | ✅ toDto/fromDto | None |
| **Gemini** | ✅ Present | ✅ Present | ✅ Present | Bug in moon query |

**Summary:** All apps properly separate business logic into services. Gemini has a runtime bug in one method that causes 500 errors.

**Why This Matters:** Services keep controllers thin and make code easier to test and maintain. The bug in Gemini shows AI can write correct structure but sometimes makes logical errors.

---

## 3. Best Practices Adherence

### 3.1 Centralized Exception Handling

**What We Tested:** When errors occur, do they return structured JSON with all required information?

**Test Used:** `PlanetControllerIntegrationTest.createPlanet_duplicateName_shouldReturn400_ErrorResponse()` and `getPlanetById_missing_shouldReturn404_ErrorResponse()`

**Results:**

| App | @RestControllerAdvice | Structured Response | Fields Present |
|-----|----------------------|---------------------|----------------|
| **Human** | ✅ Yes | ✅ ErrorResponse | ✅ timestamp, status, error, message, path |
| **GPT** | ✅ Yes | ✅ ErrorResponse | ✅ All fields (code verified) |
| **Gemini** | ✅ Yes (GlobalExceptionHandler) | ✅ Structured | ✅ All fields |

**Summary:** All three apps implement centralized exception handling correctly. The architecture is good even though GPT has runtime issues preventing full test verification.

**Why This Matters:** Good error messages help developers debug problems. Centralized handling ensures all errors follow the same format.

---

### 3.2 Controller @ResponseStatus Usage

**What We Checked:** Do controllers use `@ResponseStatus` annotations instead of returning `ResponseEntity`?

**Inspection:** Reviewed controller methods in `PlanetController`, `MoonController`

**Results:**

| App | POST Status | DELETE Status | Follows Spec |
|-----|-------------|---------------|--------------|
| **Human** | ✅ `@ResponseStatus(CREATED)` | ✅ `@ResponseStatus(NO_CONTENT)` | ✅ Yes |
| **GPT** | ✅ `@ResponseStatus(CREATED)` | ✅ `@ResponseStatus(NO_CONTENT)` | ✅ Yes |
| **Gemini** | ✅ `@ResponseStatus(CREATED)` | ✅ `@ResponseStatus(NO_CONTENT)` | ✅ Yes |

**Summary:** All three apps follow the requirement to use `@ResponseStatus` instead of `ResponseEntity`.

**Why This Matters:** The specification explicitly requires this pattern. All apps got it right.

---

## 4. Security Implementation

### 4.1 Role-Based Access Control

**What We Tested:** Can students only read data while staff and admins can modify it?

**Test Used:** `PlanetControllerIntegrationTest.student_canReadButCannotWrite()`

**Results:**

| App | Student GET | Student POST | Staff POST | Security Config |
|-----|-------------|--------------|------------|-----------------|
| **Human** | ✅ 200 OK | ❌ 403 Forbidden | ✅ 201 Created | Method-level matchers |
| **GPT** | ✅ 200 OK | ❌ 403 Forbidden | ✅ Would work | Method-level matchers |
| **Gemini** | ✅ 200 OK | ❌ 403 Forbidden | ✅ 201 Created | Method-level matchers |

**Summary:** All three apps correctly implement role-based access for REST endpoints. Students cannot modify data.

**Why This Matters:** Access control is critical for security. All apps properly restrict write operations to authorized users.

---

### 4.2 Password Encryption

**What We Checked:** Are passwords stored securely with BCrypt hashing?

**Inspection:** Reviewed `DataInitializer` / `DataLoader` and `SecurityConfig`

**Results:**

| App | Password Encoder | BCrypt Used | Passwords Hashed |
|-----|------------------|-------------|------------------|
| **Human** | ✅ Bean configured | ✅ Yes | ✅ Before saving |
| **GPT** | ✅ Bean configured | ✅ Yes | ✅ Before saving |
| **Gemini** | ✅ Bean configured | ✅ Yes | ✅ Before saving |

**Summary:** All three apps properly hash passwords with BCrypt. No plaintext passwords stored.

**Why This Matters:** Storing passwords in plain text is a major security vulnerability. All apps handle this correctly.

---

## 5. AOP Logging Implementation

### 5.1 Three Required Pointcuts

**What We Checked:** Is there an aspect with three different types of logging (controller entry/exit, service timing, exception logging)?

**Inspection:** Reviewed `LoggingAspect` class

**Results:**

| App | @Before/@AfterReturning | @Around (timing) | @AfterThrowing | Meets Spec |
|-----|-------------------------|------------------|----------------|------------|
| **Human** | ✅ Controller logging | ✅ Service timing | ✅ Exception logging | ✅ Yes |
| **GPT** | ✅ Controller logging | ✅ Service timing | ✅ Exception logging | ✅ Yes |
| **Gemini** | ✅ Controller logging | ✅ Service timing | ✅ Exception logging | ✅ Yes |

**Summary:** All three apps meet the AOP requirement with the required three pointcuts.

**Why This Matters:** AOP (Aspect-Oriented Programming) separates logging from business logic. The specification required three specific pointcuts, and all apps delivered them.

---

## 6. Overall Assessment

### Build and Run Success

| App | Builds | Runs | Tests Pass | Issues |
|-----|--------|------|------------|---------|
| **Human** | ✅ Yes | ✅ Yes | ✅ 10/10 | None |
| **GPT** | ✅ Yes | ✅ Yes | ⚠️ 6/9 | Missing 1 endpoint |
| **Gemini** | ✅ Yes | ✅ Yes | ⚠️ 6/7 | Moon query bug |

---

### Completeness vs Specification

| App | Endpoints | Security | Validation | AOP | Score |
|-----|-----------|----------|------------|-----|-------|
| **Human** | ✅ All present | ✅ Correct | ✅ Complete | ✅ Yes | 100% |
| **GPT** | ⚠️ Missing 1 endpoint | ✅ Correct | ✅ Complete | ✅ Yes | ~90% |
| **Gemini** | ⚠️ Missing 2 endpoints | ⚠️ GraphQL too open | ✅ Complete | ✅ Yes | ~85% |

---

### Code Readability

| App | Style | Comments | Maintainability |
|-----|-------|----------|-----------------|
| **Human** | Clean, professional | Well-commented | High |
| **GPT** | Verbose but clear | Minimal | Medium-High |
| **Gemini** | Very concise | Minimal | Medium |

---

## Main Findings

### Strengths of AI-Generated Code

1. **Fast scaffolding** - Both AI tools created complete project structures in minutes
2. **Good security basics** - Both implemented BCrypt, role-based access, and Basic Auth correctly
3. **Proper layering** - Both separated controllers, services, repositories, and DTOs
4. **AOP logging** - Both included the required aspect with three pointcuts
5. **Clean architecture** - Both follow Spring Boot best practices for structure

### Limitations of AI-Generated Code

1. **Missing features** - GPT missing 1 endpoint, Gemini missing 2 endpoints
2. **Runtime issues** - Gemini has a service bug in moon update logic
3. **Security gaps** - Gemini allows students to query GraphQL when spec requires admin-only
4. **No completeness check** - AI doesn't verify all spec requirements; needs human review
5. **Different patterns** - Gemini uses query params where spec shows paths, reducing consistency

---

## Why AI Made These Mistakes

### GPT's Missing Endpoint Issue
**Problem:** Missing `/api/planets/fields/name-mass` endpoint that returns only planet name and mass.

**Why:** AI likely focused on the core CRUD operations and the search endpoints, but missed this specific field-projection endpoint from the specification.

**Fix:** Add the missing endpoint to PlanetController with appropriate repository query method.

---

### Gemini's GraphQL Security Gap
**Problem:** GraphQL query endpoint allows any authenticated user, not just admins.

**Why:** AI probably learned that queries are "read operations" so doesn't restrict them as strictly as mutations (write operations).

**Fix:** Add `@PreAuthorize("hasRole('ADMIN')")` to the query method or update SecurityConfig to require admin role for `/graphql`.

---

### Missing Endpoints
**Problem:** Both AI apps are missing some required endpoints (names, name+mass).

**Why:** AI focused on core CRUD and didn't carefully check for all extra query requirements in the specification.

**Fix:** Manually review the specification and add missing endpoints with proper service and repository methods.

---

## Which AI Tool We Prefer

We would choose **GPT** for Spring Boot projects because:

1. **Better endpoint structure** - Follows standard REST patterns (path-based endpoints)
2. **Stronger security** - GraphQL properly restricted to admins
3. **More complete** - Only missing 1 endpoint vs Gemini's 2
4. **Better validation** - More thorough exception handling and validation out of the box
5. **Easier comparison** - Code structure closer to typical Spring Boot examples

However, GPT has one missing endpoint to add. Overall, GPT produced ~90% complete code vs Gemini's ~85%.

---

## Recommendations

### For Using AI to Generate Spring Boot Code

1. **Always review against the full specification** - AI may miss requirements
2. **Run tests immediately** - Catches runtime issues early
3. **Check security carefully** - AI gets basic auth right but misses fine-grained permissions
4. **Verify all endpoints** - Don't assume AI created everything in the spec
5. **Fix database initialization** - Use code-based seeding, not SQL with explicit IDs
6. **Test with all roles** - Make sure students, staff, and admins have correct access

---

### To Improve These AI Projects

**GPT:**
- Add missing `/api/planets/fields/name-mass` endpoint
- Keep GraphQL admin-only as is (it's already correct)

**Gemini:**
- Restrict GraphQL query to admin-only (add `@PreAuthorize` or update SecurityConfig)
- Fix moon query bug (investigate NPE or repository method mismatch)
- Add missing `/api/planets/names` and `/api/planets/fields/name-mass` endpoints
- Consider using path-based endpoints instead of query params for consistency

---

## Conclusion

Both AI tools can generate working Spring Boot applications quickly, but they're not perfect:

- **GPT** achieved ~90% completeness with better security and more complete endpoints
- **Gemini** achieved ~85% completeness with cleaner code but more missing features
- **Human** achieved 100% completeness with full testing and verification

### Key Takeaways

1. **AI saves time** - Gets you 85-90% of the way there in minutes vs hours
2. **AI needs verification** - Must test thoroughly and fix gaps
3. **Specs matter** - AI can miss requirements; human review is essential
4. **Security needs care** - AI gets basics right but misses nuanced access control
5. **Tests catch issues** - Integration tests reveal what AI got wrong

### For Students

Using AI to generate initial code is helpful for learning and saving time, but:
- You must understand what the code does
- You must test it thoroughly
- You must fix the gaps yourself
- You must verify it meets all requirements

To get full marks on a project, treat AI-generated code as a starting point, not a finished product. The critical analysis shows AI is good but not perfect - human oversight is still essential.

---

## Test Results Summary

### Human App
- ✅ Build: Success
- ✅ Run: Success  
- ✅ Tests Created: 10 controller integration tests
- ✅ Tests Passing: 10/10 (all pass)
- ✅ Completeness: 100%

### GPT App  
- ✅ Build: Success
- ✅ Run: Success
- ✅ Tests Created: 9 controller integration tests
- ⚠️ Tests Passing: 6/9 (3 failures - missing endpoint)
- ⚠️ Completeness: ~90% (missing 1 endpoint: `/api/planets/fields/name-mass`)

### Gemini App
- ✅ Build: Success
- ✅ Run: Success
- ✅ Tests Created: 7 controller integration tests  
- ⚠️ Tests Passing: 6/7 (1 failure - moon update bug)
- ⚠️ Completeness: ~85% (missing 2 endpoints, GraphQL security gap)

### Note on Test Count Differences
We focused on controller-level integration tests as they provide the most meaningful comparison:
- Human app uses Builder pattern with Lombok
- GPT app uses plain setters without Lombok  
- Gemini app uses Lombok BUT has different service method names and repository query methods

Controller tests (which we successfully ran on all 3 apps) provide the best comparison since they test the external REST API behavior that users actually interact with.

